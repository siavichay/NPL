{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador de Sentimientos (Like-Dislike) \n",
    "==\n",
    "con Naive Bayes\n",
    "===\n",
    "\n",
    "<img src=\"imagenes/like-dislike.png\" height=\"375\" width=\"193\">\n",
    "\n",
    "\n",
    "Por Alexander Siavichay.\n",
    "\n",
    "Una versión más actualizada de este documento, se puede encontrar en:\n",
    "https://github.com/siavichay/NPL/blob/master/Clasificador_de_Sentimientos_(Like_Dislike)_Naive_Bayes.ipynb\n",
    "\n",
    "Documento adaptado de Spmaclasifier (Bayesian_Inference).\n",
    "\n",
    "Hoy en día, la web, y en especial las redes sociales, han crecido de manera exponencial.Con el pasar del tiempo aumenta la variedad de usos de los datos que fluyen en estos medios en diferentes ámbitos, como: el social, negocios, marketing, generador de noticias, diversión entre otros [Redes Sociales](https://www.investopedia.com/terms/s/social-networking.asp).\n",
    "\n",
    "Es necesario conocer lo que los usuarios hacen al interactuar en estos medios, pero al ser millones, sería imposible hacer una estadística manual o realizar otras operaciones de minería de datos, big data, para conocer las opiniones sobre una publicación en particular. Sin el uso de la tecnología y técnicas como la bayesiana sería una tarea casi imposible.\n",
    "\n",
    "Nota: Este algoritmo fue probado previamente para la clasificación de spam.\n",
    "\n",
    "Hemos decidido tomar información real de comentarios de usuarios de diferentes sitios web.\n",
    "\n",
    "La base de datos ha sido tomada del Paper 'From Group to Individual Labels using Deep Features' , Kotzias et. al,. KDD 2015.\n",
    "\n",
    "Esta información está etiquetada como sentimientos positivo o negativo, extraídos de vista a productos, películas, y restaurantes.\n",
    "\n",
    "\n",
    "\n",
    "Formato\n",
    "=======\n",
    "centencia \\t puntuación \\n\n",
    "\n",
    "\n",
    "Detalles\n",
    "=======\n",
    "La puntuación es 1 (positivo) o 0 (negativo)\n",
    "\n",
    "Las críticas se han tomado de sitios web:\n",
    "\n",
    "imdb.com\n",
    "amazon.com\n",
    "yelp.com\n",
    "\n",
    "La muestra corresponde a oraciones 500 positivas y 500 negativas. Estas muestras se han seleccionado de manera aleatoria.\n",
    "Las oraciones son claramentes positivas o negativas, evitando opiniones neutrales.\n",
    "\n",
    "La base de datos completa se puede seleccionar desde:\n",
    "\n",
    "imdb: Maas et. al., 2011 'Learning word vectors for sentiment analysis'.\n",
    "\n",
    "amazon: McAuley et. al., 2013 'Hidden factors and hidden topics: Understanding rating dimensions with review text'.\n",
    "\n",
    "yelp: [Yelp dataset challenge] (http://www.yelp.com/dataset_challenge).\n",
    "\n",
    "\n",
    "Nuestro objetivo es utilizar el algoritmo Naive Bayes para crear un modelo que clasifique las opiniones de visitantes a los sitios web mencionados. Se ha filtrado una base de palabras para las opiniones negativas tomadas del dataset, entre las cuales hemos observado:\n",
    "\n",
    "\n",
    "El presente análisis es de tipo binario like(positivo-->1), dislike(negativo-->0), no existe una tercera opción. Es un problema de aprendizaje supervisado, puesto que, se lo está alimentando con un dataset al modelo, del cual aprenderá, para las futuras predicciones.\n",
    "\n",
    "\n",
    "# Capitulación\n",
    "\n",
    "Para este análsis se abordarán los siguientes temas\n",
    "\n",
    "- Paso 0: Introducción al teorema de Naive Bayes\n",
    "- Paso 1.1: Comprendiendo el dataset\n",
    "- Paso 1.2: Pre-Procesamiento de datos\n",
    "- Paso 2.1: Bolsa de Palabras(BoW)\n",
    "- Paso 2.2: Implentando BoW desde Cero\n",
    "- Paso 2.3: IImplentando BoW  con scikit-learn\n",
    "- Paso 3.1: Construyendo Training y testing\n",
    "- Paso 3.2: Aplicando el procesamiento Bag of Words a nuestros datos.\n",
    "- Paso 4.1: Implementación del teorema de Bayes desde cero\n",
    "- Paso 4.2: Implementación de Naive Bayes desde cero\n",
    "- Paso 5: Implementando Naive Bayes usando scikit-learn\n",
    "- Paso 6: Evaluación de nuestro modelo\n",
    "- Paso 7: Conclusión\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 0: Introducción al teorema de Naive Bayes ###\n",
    "\n",
    "El [teorema de Bayes](https://www.investopedia.com/terms/b/bayes-theorem.asp) es uno de los algorimos de inferencia probabilística desarrollado por Revrend Bayes, que un inicio las pretenciones de este algoritmo eran determinar la existencia de Dios, pero hoy por hoy, trabaja muy bien para una variedad de casos. \n",
    "\n",
    "<img src=\"imagenes/experimento1.jpg\" height=\"453\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    "Diagrama de árbol según el experimento\n",
    "\n",
    "<img src=\"imagenes/arbol_probabilidades_evento1.jpg\" height=\"342\" width=\"342\">\n",
    "\n",
    "\n",
    "¿Cual es la probabilidad que la bolita del segundo buzón sea roja, si la bolita del primer buzón salió negra?\n",
    "\n",
    "<img src=\"imagenes/expresion_inicial_2.jpg\" height=\"515\" width=\"222\">\n",
    "\n",
    "\n",
    "Descripción del Resultado\n",
    "\n",
    "<img src=\"imagenes/enlaza_formula_resultado.jpg\" height=\"550\" width=\"241\">\n",
    "\n",
    "\n",
    "El teormea de Bayes trabaja muy bien al computar la probabilidad de un evento basado en las probabilidades de ciertos eventos relativos.\n",
    " \n",
    "Para mayor información visite [¿Cómo entender el Teorema de Bayes en forma simple?](https://www.docirs.cl/entender_teorema_de_bayes_simple.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.1: Comprendiendo el dataset ### \n",
    "\n",
    "Se utilizará el [dataset](http://www.yelp.com/dataset_challenge) del Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015.\n",
    "\n",
    "Este primer paso es muy importante, no solo está el hecho de obtener un dataset, lo primero es tener claro la visión de lo que se pretende obtener, con ello, buscar el dataset que contenga los datos que permitan obtener la información objetivo. Si es que no es posible utilizar el dataset directamenten con la librería pandas, debemos filtrar el dataset a un formato legible por la librería que estamos utilizando.\n",
    "\n",
    " ** Vista de los datos ** \n",
    "\n",
    "<img src=\"imagenes/muestra.png\" height=\"1242\" width=\"1242\">\n",
    "\n",
    "Las columnas no tienen nombre, sin embargo podemos identificar dos.\n",
    "\n",
    "La primera columna es la opinión del usuario y la segunda es la valoración 1(mensaje positivo) y 0(mensaje negativo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">** Procedimiento: **\n",
    "* Se importa el dataset en un dataframe pandas usando el método read_csv. Because this is a tab separated dataset we will be using '\\t' as the value for the 'sep' argument which specifies this format. \n",
    "* También, se renombra las columnas con una lista ['opinion, 'valor'] a los 'names' como argumento de of read_csv().\n",
    "* Se imprime los primeros valores del  dataframe con el nombre de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             opinion  sentimiento\n",
       "0                           Wow... Loved this place.            1\n",
       "1                                 Crust is not good.            0\n",
       "2          Not tasty and the texture was just nasty.            0\n",
       "3  Stopped by during the late May bank holiday of...            1\n",
       "4  The selection on the menu was great and so wer...            1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Procedimiento\n",
    "'''\n",
    "import pandas as pd\n",
    "# Dataset tomado de - http://www.yelp.com/dataset_challenge\n",
    "df = pd.read_csv('dataset/sentimientos.txt', names=['opinion', 'sentimiento'],sep='\\t')\n",
    "# Output printing out first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.2: Pre-Procesamiento de datos ###\n",
    "\n",
    "En este caso, no es necesario manejar las entradas para la librería scikit-learn, pues, los valores en el dataset están normalizados a 0 y 1 respectivamente, en la mayoría de casos debemos hacer un preprocesamiento de los datos para que la librería pueda manejarlos.\n",
    "\n",
    "Es necesario que las comparaciones sean numéricas, ya que al final necesitamos calcular la precisión, recall, entre otros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.1: Bolsa de Palabras(BoW) ###\n",
    "\n",
    "La mayoría de algoritmos son numéricos. En nuestro dataset, como en la mayoría, nos encontraremos con textos.\n",
    "\n",
    "Bolsa de Palabras, del inglés Bag of Words(BoW), es un término que refiere a problemas en los cuales tenemos un conjunto de palabras, la idea, es tomar parte de este texto y contar la frecuencia de las palabras en ese texto. El orden en el que aparecen no es de interés, solo la frecuencia.\n",
    "\n",
    "El objetivo es construir una martriz, cada documentos será una fila y cada palabra una columna, con los valores de la frecuencia.\n",
    "\n",
    "Por ejemplo: \n",
    "\n",
    "Si tomamos 4 textos\n",
    "\n",
    "`['Hello, how are you!',\n",
    "'Win money, win from home.',\n",
    "'Call me now',\n",
    "'Hello, Call you tomorrow?']`\n",
    "\n",
    "Nuestro objetivo es convertir este conjunto de texto a matriz de distribución de frecuencia:\n",
    "\n",
    "\n",
    "<img src=\"imagenes/countvectorizer.png\" height=\"542\" width=\"542\">\n",
    "\n",
    "Cada frase pertenece a una fila, y cada palabra a un nombre de la columna, con el valor de la frecuencia que le corresponde a esa frecuencia.\n",
    "\n",
    "\n",
    "\n",
    "Lets break this down and see how we can do this conversion using a small set of documents.\n",
    "\n",
    "Se utilizara el método scikit-learn:\n",
    "\n",
    "[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) \n",
    "\n",
    "* Crea un token para la cadena de texto(separa en palabras al texto)It tokenizes the string(separates the string into individual words) y le dá un ID numperico.\n",
    "* Cuenta la frecuencia de los tokens.\n",
    "\n",
    "** Tomar nota ** \n",
    "\n",
    "* El método CountVectorizer automáticamente convierte las palabras con tokens a minúsculas usando el parámetro `lowercase` que por defecto está en `True`.\n",
    "\n",
    "* Ignora las puntuaciones. Utiliza el parámetro  `token_pattern`  que tiene una expresión regular que selecciona 2 tokens con caracteres alfanuméricos.\n",
    "\n",
    "* El tercer parámetro `stop_words`, se refiere a las palabra más comunmente usadas en un lenguaje. Ejemplo: 'de', 'un', 'y', 'el' etc.  En nuestro caso especificaremos el `english`, CountVectorizer ignorará todas las palabras de manera automática del dataset. Es indispensable utilizar los stop words para mejorar los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.2: Implentando BoW desde Cero ###\n",
    "\n",
    "Antes de usar la librería de BoW scikit-learn, vamos a hacer el proceso para saber como funciona este algoritmo.\n",
    "\n",
    "**Paso 1: Pasar a minúsculas el texto.**\n",
    "Se puede utilizar el método lower() de Phyton.\n",
    "\n",
    "Luego de analizar el dataset, vamos a trabajar con el siguientes opiniones:\n",
    "\n",
    "```\n",
    "opiniones = ['Wow... Loved this place.',\n",
    "             'Crust is not good.',\n",
    "             'Not tasty and the texture was just nasty.',\n",
    "             'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.']\n",
    "```\n",
    "*Se guardarán los resultados en lower_case_opiniones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow... loved this place.', 'crust is not good.', 'not tasty and the texture was just nasty.', 'stopped by during the late may bank holiday off rick steve recommendation and loved it.']\n"
     ]
    }
   ],
   "source": [
    "opiniones = ['Wow... Loved this place.',\n",
    "             'Crust is not good.',\n",
    "             'Not tasty and the texture was just nasty.',\n",
    "             'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.']\n",
    "\n",
    "lower_case_opiniones = []\n",
    "for i in opiniones:\n",
    "    lower_case_opiniones.append(i.lower())\n",
    "print(lower_case_opiniones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 1: Remover las puntuaciones.**\n",
    "Se guardará el resultadno en una lista llamada 'sans_punctuation_documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow loved this place', 'crust is not good', 'not tasty and the texture was just nasty', 'stopped by during the late may bank holiday off rick steve recommendation and loved it']\n"
     ]
    }
   ],
   "source": [
    "sans_punctuation_opiniones = []\n",
    "import string\n",
    "\n",
    "for i in lower_case_opiniones:\n",
    "    sans_punctuation_opiniones.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "    \n",
    "print(sans_punctuation_opiniones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3: Asignar Tokens**\n",
    "\n",
    "a. Se separan las palabras de cada opinión usando un delimitador\n",
    "b. El delimitador identifica el incio y fin de cada palabra (Puede ser un espacio, como es nuestro caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El resultado se guardará en 'sans_punctuation_opiniones' utilizando el método split(). Luego el resultado final se almacenará en 'preprocessed_opiniones'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wow', 'loved', 'this', 'place'], ['crust', 'is', 'not', 'good'], ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty'], ['stopped', 'by', 'during', 'the', 'late', 'may', 'bank', 'holiday', 'off', 'rick', 'steve', 'recommendation', 'and', 'loved', 'it']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_opiniones = []\n",
    "for i in sans_punctuation_opiniones:\n",
    "    preprocessed_opiniones.append(i.split())\n",
    "print(preprocessed_opiniones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4: Contar las frecuencias **\n",
    "a. Se cuenta la ocurrencia de cada cada palabra en cada fila(opinión). Se utilizará el método `Counter` de la librería `collections`.\n",
    "\n",
    "`Counter` cuenta el número de ocurrencias y retorna un diccionario con las claves del item contado y con el valor de conteo respectivo en la lista. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Se utiliza el método Counter() y se graba el diccionario en una lista llamada 'frequency_list'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'wow': 1, 'loved': 1, 'this': 1, 'place': 1}),\n",
      " Counter({'crust': 1, 'is': 1, 'not': 1, 'good': 1}),\n",
      " Counter({'not': 1,\n",
      "          'tasty': 1,\n",
      "          'and': 1,\n",
      "          'the': 1,\n",
      "          'texture': 1,\n",
      "          'was': 1,\n",
      "          'just': 1,\n",
      "          'nasty': 1}),\n",
      " Counter({'stopped': 1,\n",
      "          'by': 1,\n",
      "          'during': 1,\n",
      "          'the': 1,\n",
      "          'late': 1,\n",
      "          'may': 1,\n",
      "          'bank': 1,\n",
      "          'holiday': 1,\n",
      "          'off': 1,\n",
      "          'rick': 1,\n",
      "          'steve': 1,\n",
      "          'recommendation': 1,\n",
      "          'and': 1,\n",
      "          'loved': 1,\n",
      "          'it': 1})]\n"
     ]
    }
   ],
   "source": [
    "frequency_list = []\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "for i in preprocessed_opiniones:\n",
    "    frequency_list.append(Counter(i))\n",
    "    \n",
    "pprint.pprint(frequency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí, hemos implementado el proceso Bow. Como se puede observar en la matriz, tenemos cada palabra con la frecuencia respectiva.\n",
    "\n",
    "Este proceso es realizado de manera automática por el método `sklearn.feature_extraction.text.CountVectorizer`.\n",
    "\n",
    "A continuación se implementará el método `sklearn.feature_extraction.text.CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.3: Implentando BoW con scikit-learn ###\n",
    "\n",
    "En este paso se implementará directamanet el proceso de scikit-learn, se utilizará el mismo texto que en el caso de la implementaicón manual en el paso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opiniones = ['Wow... Loved this place.',\n",
    "             'Crust is not good.',\n",
    "             'Not tasty and the texture was just nasty.',\n",
    "             'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el método sklearn.feature_extraction.text.CountVectorizer y se crea la instancia 'count_vector'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Preprocesamiento con CountVectorizer() **\n",
    "\n",
    "En el paso 2.2, se implementó el método CountVectorizer(). En este caso se pasa a minúsculas y se remueven las puntuaciones. CountVectorizer() recibe ciertods parámetros que se esepecifican a continuación:\n",
    "\n",
    "* `lowercase = True`\n",
    "    \n",
    "     `lowercase` tiene asignado `True` por defecto para convertir a minúsculas.\n",
    "\n",
    "\n",
    "* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n",
    "    \n",
    "     `token_pattern` recibe una expresión regular `(?u)\\\\b\\\\w\\\\w+\\\\b` para eliminnar las puntuaciones.\n",
    "\n",
    "\n",
    "* `stop_words`\n",
    "\n",
    "    `stop_words` , esta puesto en `english`, como  se mencionó al inicio de este documento, removerá aquellas palabras que no aportan al análisis, como estamos tratando con opiniones, existirán artículos o palabras gramaticales que generarán ruido..\n",
    "\n",
    "Se imprime finalmente  `count_vector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Se envía el dataset al objeto CountVectorizer creado con el método fit(), luego, se obtiene la lista de palbras categorizadas con el método get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'bank',\n",
       " 'by',\n",
       " 'crust',\n",
       " 'during',\n",
       " 'good',\n",
       " 'holiday',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'late',\n",
       " 'loved',\n",
       " 'may',\n",
       " 'nasty',\n",
       " 'not',\n",
       " 'off',\n",
       " 'place',\n",
       " 'recommendation',\n",
       " 'rick',\n",
       " 'steve',\n",
       " 'stopped',\n",
       " 'tasty',\n",
       " 'texture',\n",
       " 'the',\n",
       " 'this',\n",
       " 'was',\n",
       " 'wow']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(opiniones)\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_feature_names()` nos devuelve los features(palabras), tomadas del dataset de opiniones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se crea la matriz de las 4 opiniones, y las columnas que corresponden a cada palabra.\n",
    "El valor (fila,columna) es la frecuencia de ocurrencia de cada palabra especificada en cada columna, que corresponde a las opiniones en el dataset. Se utiliza el método 'trasnform()' que se le pasa como argumento la matriz 'opiniones'. El método transform() debuelve números enteros numpy, por lo que se convierte a un arreglo utilizando toarray(), se le asigna el nombre doc_array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array = count_vector.transform(opiniones).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se convierten a un arreglo en un dataframe con el nombre de columnas apropiado, para un mejor tratamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El arreglo obtenido se carga en 'doc_array', y se combinan con los nombres de las columnas obtenidas con el método get_feature_names(), el resultado de esta operación se almacena en 'frequency_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>bank</th>\n",
       "      <th>by</th>\n",
       "      <th>crust</th>\n",
       "      <th>during</th>\n",
       "      <th>good</th>\n",
       "      <th>holiday</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>just</th>\n",
       "      <th>...</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>rick</th>\n",
       "      <th>steve</th>\n",
       "      <th>stopped</th>\n",
       "      <th>tasty</th>\n",
       "      <th>texture</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  bank  by  crust  during  good  holiday  is  it  just  ...  \\\n",
       "0    0     0   0      0       0     0        0   0   0     0  ...   \n",
       "1    0     0   0      1       0     1        0   1   0     0  ...   \n",
       "2    1     0   0      0       0     0        0   0   0     1  ...   \n",
       "3    1     1   1      0       1     0        1   0   1     0  ...   \n",
       "\n",
       "   recommendation  rick  steve  stopped  tasty  texture  the  this  was  wow  \n",
       "0               0     0      0        0      0        0    0     1    0    1  \n",
       "1               0     0      0        0      0        0    0     0    0    0  \n",
       "2               0     0      0        0      1        1    1     0    1    0  \n",
       "3               1     1      1        1      0        0    1     0    0    0  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array, columns=count_vector.get_feature_names())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momento, hemos implementado el método Bow con la librería.\n",
    "Vale la pena recalcar que están incluidas palabras que no aportan de manera positva al análisis, como son, del inglés 'is', 'the', 'an', afectando nuestro análisis.\n",
    "\n",
    "Existen dos formas de mitigar este problema, como se mencionó con el parámetro `stop_words` y el idioma correspondiente `english`. \n",
    "\n",
    "Una segunda opción es utilizar el método [tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3.1: Construyendo Training y testing###\n",
    "\n",
    "Separaremos los datos obtenidos en conjuntos de training y testing, con ello podemos hacer las pruebas de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará el método train_test_split method de la librería sklearn. Los datos quedarán registrados en las siguientes variables.\n",
    "* `X_train` datos para training que contendrá la columna de opiniones 'opiniones' .\n",
    "* `y_train` datos para training tomada de la columna 'sentimiento'.\n",
    "* `X_test` datos para testing que contendrá la columna de opiniones 'opiniones' .\n",
    "* `y_test` datos para testing tomada de la columna 'sentimiento'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de filas: 1000\n",
      "Número total de filas del conjunto training: 750\n",
      "Número total de filas del conjunto test: 250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['opinion'], \n",
    "                                                    df['sentimiento'], \n",
    "                                                    random_state=1)\n",
    "\n",
    "print('Número total de filas: {}'.format(df.shape[0]))\n",
    "print('Número total de filas del conjunto training: {}'.format(X_train.shape[0]))\n",
    "print('Número total de filas del conjunto test: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3.2: Aplicando el procesamiento Bag of Words a nuestros datos. ###\n",
    "\n",
    "Una vez separados los datos, el siguiente proceso es aplicar lo implementado en el paso 2 (Bow), y formatear en una matriz los datos. Se utilizará el objeto CountVectorizer() tal cual el paso previo. \n",
    "\n",
    "Se deben considerar 2 pasos:\n",
    "\n",
    "* Primero, tratar los datos de training(`X_train`) en `CountVectorizer()` y tomar la matriz.\n",
    "* Segundo, transformar los datos de test (`X_test`) y tomar la matriz. \n",
    "\n",
    "    `X_train` son los datos de la columna 'opinion' que se utilizará para el entrenamiento de nuesro modelo. \n",
    "\n",
    "    `X_test` son los datos de la columna 'opinion' que se utilizarán en el futuro apara realizar las predicciones. Luego se compararán los resultados con los datos obtenidos en  `y_test` en un paso posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl código para este segmento está en 2 partes. \\nPrimero, se está aprendiendo un diccionario para los datos de training Luego se transforman en una matris opiniones-term;\\nSegundo, para los datos de testing se transforman los datos de opiniones-term.\\n\\nSimilar al paso 2.3\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "El código para este segmento está en 2 partes. \n",
    "Primero, se está aprendiendo un diccionario para los datos de training Luego se transforman en una matris opiniones-term;\n",
    "Segundo, para los datos de testing se transforman los datos de opiniones-term.\n",
    "\n",
    "Similar al paso 2.3\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el método CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Tratar los datdos de training y tomar la matriz\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Tratar los datdos de testing y tomar la matriz.\n",
    "testing_data = count_vector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Implementación del teorema de Bayes desde cero ###\n",
    "\n",
    "Con los datos formateados de nuestro dataset, podemos proseguir con las predicciones para clasificar las opiniones como positivas(like) o negativas(dislike). El teorema de Bayes calcula la probabilidad de un evento que ocurre, en base a ciertas probabilidades de otro eventos rellacionados. El pundo de partida son las probabilidades que tenemos, y luego las propabbilidades calculadas en base a las primeras.\n",
    "\n",
    "Por ejemplo: si tratamos de identificar la probabilidade de que una persona tenga Diabetes. Asumimos lo siguiente\n",
    "`P(D)` es la probabilidad de que una persona tiene Diabetes.\n",
    "\n",
    "Su valor es de `0.01` (1%) del valor general\n",
    "\n",
    "`P(Pos)` es la probabilidad de obtener un resultado positivo.\n",
    "\n",
    "`P(Neg)` es la probabilidad de obtener un resultado negativo.\n",
    "\n",
    "`P(Pos|D)`  es la probabilidad de obtener un resultado positivo dado que la persona tiene diabetes.\n",
    "Este valor es de `0.9` (90%), también se le conoce como la Sensitividad o RAtio Verdadero Positivo\n",
    "\n",
    "`P(Neg|~D)` es la probabilidad de obtener un resultado negativo dado que la persona No tiene diabetes.\n",
    "Este valor también es de `0.9` (90%), también se le conoce como la Especificidad o RAtio Verdadero Negativo\n",
    "\n",
    "La fórmula de Bayes es entonces:\n",
    "\n",
    "<img src=\"images/bayes_formula.png\" height=\"242\" width=\"242\">\n",
    "\n",
    "* `P(A)`  es la propbaiblidad inicial de un evento A independiente. En nuestro ejemplo `P(D)`. Es un valor que se nos da.\n",
    "\n",
    "* `P(B)`  es la propbaiblidad inicial de un evento B independiente. En nuestro ejemplo `P(Pos)`.\n",
    "\n",
    "* `P(A|B)` es la probabilidade posterior de A dado que ocurrió B. En nuestro ejemplo, `P(D|Pos)`. \n",
    "**La probabilidad de que un individuo tenga Diabetes, dado que, el individuo tiene un resultado positivo. Es el valor que queremos calcular.**\n",
    "\n",
    "* `P(B|A)` es la probabilidad e que ocurra B, dado que se dió A. En nuestro ejemplo  `P(Pos|D)`. Es un valor que se nos da."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos los valores en la fórmulad e Bayes:\n",
    "\n",
    "\n",
    "`P(D|Pos) = P(D) * P(Pos|D) / P(Pos)`\n",
    "\n",
    "La probabilidad de obtener un resultado positivo `P(Pos)` puede calcularse usando la sensitividad y Especificidad como sigue:\n",
    "\n",
    "`P(Pos) = [P(D) * sensitividad] + [P(~D) * (1-Especificidad))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calculamos la probabilidad de obtener un resultado positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilidad de obtener un resultado Positivo es P(Pos) is: {} 0.10799999999999998\n"
     ]
    }
   ],
   "source": [
    "# P(D)\n",
    "p_diabetes = 0.01\n",
    "\n",
    "# P(~D)\n",
    "p_no_diabetes = 0.99\n",
    "\n",
    "# Sensitivity or P(Pos|D)\n",
    "p_pos_diabetes = 0.9\n",
    "\n",
    "# Specificity or P(Neg|~D)\n",
    "p_neg_no_diabetes = 0.9\n",
    "\n",
    "# P(Pos)\n",
    "p_pos = (p_diabetes * p_pos_diabetes) + (p_no_diabetes * (1 - p_neg_no_diabetes))# TODO\n",
    "print('La probabilidad de obtener un resultado Positivo es P(Pos) is: {}',format(p_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora con la información podemos calcular resultados posteriores**\n",
    "\n",
    "La probabilidad de que un individuo que tiene diabete, dado que, tiene un resultado positivo es:\n",
    "\n",
    "`P(D|Pos) = (P(D) * Sensitivity)) / P(Pos)`\n",
    "\n",
    "La probabilidad de que un individuo que No tiene diabete, dado que, tiene un resultado positivo es:\n",
    "\n",
    "\n",
    "`P(~D|Pos) = (P(~D) * (1-Specificity)) / P(Pos)`\n",
    "\n",
    "La suma, debe ser  `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Calculamos la probabilidad de que un individuo que tiene diabetes, dado que, ese individuo tiene un resultado positivo. \n",
    " P(D|Pos).\n",
    "\n",
    "La fórmula es: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilidad de que un individuo que tiene diabets, dado que tiene un resultado positivo es: 0.08333333333333336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_diabetes_pos = (p_diabetes * p_pos_diabetes) / p_pos # TODO\n",
    "print('La probabilidad de que un individuo que tiene diabets, dado que tiene un resultado positivo es:\\\n",
    "',format(p_diabetes_pos)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos ahora la probabilidad de un individuo que no tiene diabetes, dado que, ese individuo tiene un resultado positivo\n",
    " P(~D|Pos).\n",
    "\n",
    "La fórmula es: P(~D|Pos) = P(~D) * P(Pos|~D) / P(Pos)\n",
    "\n",
    "P(Pos|~D) puede determinarse como 1 - P(Neg|~D). \n",
    "\n",
    "Entonces:\n",
    "P(Pos|~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la probabilidad de un individuo que no tiene diabetes, dado que, ese individuo tiene un resultado positivo: 0.9166666666666669\n"
     ]
    }
   ],
   "source": [
    "p_pos_no_diabetes = 0.1\n",
    "\n",
    "# P(~D|Pos)\n",
    "p_no_diabetes_pos = (p_no_diabetes * p_pos_no_diabetes) / p_pos\n",
    "print ('la probabilidad de un individuo que no tiene diabetes, dado que, ese individuo tiene un resultado positivo:'\\\n",
    ",p_no_diabetes_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos implementado el teorema de Bayes. Hemos demostrado que, si se obtiene un resultado positivo, solo hay el 8,3% de que la persona tiene diabetes y un 91,67% de que no tiene diabetes. Asumiendo de que solo el 1% de la población tiene diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ¿De donde procede 'Naive' en 'Naive Bayes'?** \n",
    "\n",
    "El término 'Naive' en Naive Bayes viene del hecho que el algoritmo considera las características que se utilizan para realizar las predicciones independiente una de otra, no siempre es asi. Naive es una extensión de Bayes que asume que todas las características son independientes.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4.2: Implementación de Naive Bayes desde cero ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideraremos más de una variable\n",
    "\n",
    "quedando de la siguiente manera:\n",
    "\n",
    "<img src=\"imagenes/naivebayes.png\" height=\"342\" width=\"342\">\n",
    "\n",
    " `y` es el candidato  y `x1` a `xn` son los features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Implementando Naive Bayes usando scikit-learn ###\n",
    "\n",
    "Sklearn tiene varias implementaciones de Naive Bayes que podemos utilizar. Utilizaremos el método `sklearn.naive_bayes` para hacer las predicciones de nuestro dataset.\n",
    "\n",
    "Especificacmente, se utilizará la implementacion multinomial Naive Bayes. Factible con variables discreta (en nuestro caso, el conteo de palabras para clasificar el texo). Toma como entrada un número entero que representa el conteo de una palabra. Naive Bayes Gauseano es mejor para variable sontinuas con una distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que se ha cargado los datos de  training en 'training_data'y los de testing en 'testing_data'.\n",
    "\n",
    "Importamos el clasificador MultinomialNB, se moldean los datos de entreganimeinto usando fit(). Se nombra el clasificador\n",
    "Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\n",
    "'naive_bayes'. Se entrena´ra utilizando 'training_data' y y_train'. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos entrenado nuestro algoritmo usando training_data, podemos realizar predicciones en 'testing_data' utilizando predict(). Guardamos las predicciones en la variable 'predictions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las predicciones en nuestro arreglo test, necesitamos calcular el acuuracy de nuestras predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6: Evaluación de nuestro modelo ###\n",
    "\n",
    "Ahora que tenemos las predicciones en nuestro test, debemos evaluar nuestro modelo. \n",
    "\n",
    "** Accuracy ** mide cuan frecuente nuestro clasificador hace una preción correcta. Es el número de predicciones correctas del total del número de predicciones\n",
    "\n",
    "\n",
    "** Precision ** nos dice la proporción de opiniones que clasificamos como negativas, y que realmente lo son.\n",
    "Es un ratio de verdaderos positivos\n",
    "\n",
    "`[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "** Recall(sensitivity)** nos muestra la proporcion de mensajes que actualmente son negativos de los negativos. \n",
    "Es el ratio de los verdaderos positivos, con respecto a los que realmente son:\n",
    "\n",
    "`[True Positives/(True Positives + False Negatives)]`\n",
    "En nuestro caso, si tenemos 100 opiniones y solo 2 fueron negativos,  los 98 no lo fueron, el accuracy no es una métrica efectiva. Podemos clasificar 90 mensajes positivos (incluyendo los 2 que fueron negativos, pero los clasificamos como positivos) y 10 como negativos (todos los 10 son falsos positivos) pudiendose obtener un buen porcentaje de accuracy. Para este caso precision y recall nos ayudan. Estas métricas pueden combinarse para obtener el score F1, que es un promedio entre la presición y recall, entre 0 a 1, donde uno es el mejor F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos 4 métricas para asegurar que nuestro model trabaja bien. Valores entre 0 a 1, mientras más se acerquen a uno, será mejor nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculará el accuracy, precision, recall and F1  de nuestro modelo utilizando los datos de test 'y_test' and las predicciones, en la variable 'predictions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.768\n",
      "Precision :  0.7457627118644068\n",
      "Recall :  0.7586206896551724\n",
      "F1 :  0.752136752136752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision : ', format(precision_score(y_test, predictions)))\n",
    "print('Recall : ', format(recall_score(y_test, predictions)))\n",
    "print('F1 : ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Paso 7: Conclusiones ###\n",
    "\n",
    "Cuando nesecitamos realizar operaciones de clasificación, Naive Bayes nos permite utilizar varios features. En nuestro caso, cada palabra fué tratada como feature, y existen miles de palabras diferentes. Si quitamos las palabras de parada, los resultados mejoran mucho. Otra ventaja es la simplicidad. Es un algoritmo rápido y las predicciones obtenidas son buenas.\n",
    "\n",
    "En la primera prueba obtuvimos:\n",
    "Accuracy:  0.768\n",
    "Precision :  0.7457627118644068\n",
    "Recall :  0.7586206896551724\n",
    "F1 :  0.752136752136752\n",
    "\n",
    "Quitando las palabras de parada...\n",
    "\n",
    "Sería interesante utilizar este algoritmo con un dataset mucho más grande y comparar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
